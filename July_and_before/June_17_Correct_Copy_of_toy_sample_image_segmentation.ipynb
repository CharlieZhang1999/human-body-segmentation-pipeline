{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDnnjG2HuEbN",
    "outputId": "0245f1d9-3d06-4d66-b1bc-3d05b1527d30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation-models-pytorch in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (0.1.3)\r\n",
      "Requirement already satisfied: timm==0.3.2 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.3.2)\r\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.6.3)\r\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.7.4)\r\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.10.0)\r\n",
      "Requirement already satisfied: torch in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.9.0)\r\n",
      "Requirement already satisfied: munch in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\r\n",
      "Requirement already satisfied: tqdm in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.61.1)\r\n",
      "Requirement already satisfied: typing_extensions in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.10.0.0)\r\n",
      "Requirement already satisfied: numpy in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.20.3)\r\n",
      "Requirement already satisfied: pillow>=5.3.0 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (8.2.0)\r\n",
      "Requirement already satisfied: six in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucFNkiaXr-cb",
    "outputId": "7700a525-7675-4b62-f31f-48cfa43a507b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
      "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-2bkvx0ir\n",
      "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-2bkvx0ir\n",
      "Requirement already satisfied, skipping upgrade: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.10.0+cu102)\n",
      "Requirement already satisfied, skipping upgrade: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.6.3)\n",
      "Requirement already satisfied, skipping upgrade: timm==0.3.2 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.9.0+cu102)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (1.15.0)\n",
      "Building wheels for collected packages: segmentation-models-pytorch\n",
      "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.1.3-cp37-none-any.whl size=83178 sha256=fcc0a40d4e807e7ca9135315cb9775a5de553db364bd32c10a6889b56a8de28e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fmloz17d/wheels/79/3f/09/1587a252e0314d26ad242d6d2e165622ab95c95e5cfe4b942c\n",
      "Successfully built segmentation-models-pytorch\n",
      "Installing collected packages: segmentation-models-pytorch\n",
      "  Found existing installation: segmentation-models-pytorch 0.1.3\n",
      "    Uninstalling segmentation-models-pytorch-0.1.3:\n",
      "      Successfully uninstalled segmentation-models-pytorch-0.1.3\n",
      "Successfully installed segmentation-models-pytorch-0.1.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install git+https://github.com/qubvel/segmentation_models.pytorch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aaY7znN5wftY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYkECNWsrGpB",
    "outputId": "bbb5bf08-f332-4d78-ca62-aa605b96d127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cds8KIvTxL93"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/content/drive/MyDrive/Henry_Lab/Cap08102020_seg/'\n",
    "\n",
    "x_finetune_dir = '/playpen1/qiuyang/refined_segmentation/pascal_image_50'\n",
    "y_finetune_dir = '/playpen1/qiuyang/refined_segmentation/pascal_task_50'\n",
    "x_test_dir = '/playpen1/qiuyang/refined_segmentation/pascal_image_400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FARn7bq-x6bS",
    "outputId": "e94717d9-81a9-46af-91b5-e861350b2563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "June_17_Correct_Copy_of_toy_sample_image_segmentation.ipynb  \u001b[0m\u001b[01;34mpascal_image_50\u001b[0m/\r\n",
      "June_6_sample_image_segmentation.ipynb                       \u001b[01;34mpascal_mask_50\u001b[0m/\r\n",
      "\u001b[01;34mpascal_image_400\u001b[0m/                                            \u001b[01;34mpascal_refined\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pSCIdc7tx8V1"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def bbox2_padding(img, mask):\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    height = rmax - rmin + 1\n",
    "    #print(f\"height is {height}\")\n",
    "                      \n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    width = cmax - cmin + 1\n",
    "    #print(f\"width is {width}\")\n",
    "\n",
    "    rcenter = int((rmin+rmax) / 2)\n",
    "    ccenter = int((cmin+cmax) / 2)\n",
    "    \n",
    "    half_length = max(abs(rcenter - rmin), abs(ccenter-cmin))\n",
    "    length = max(height, width)\n",
    "    # initialize new_image and new_mask\n",
    "    new_image = np.zeros((length, length, 3), dtype=np.int32)\n",
    "    new_mask = np.zeros((length, length, mask.shape[2]), dtype=np.int32)\n",
    "    new_center = half_length\n",
    "\n",
    "    start_row = max(0,new_center-int(height/2))\n",
    "    start_col = max(0, new_center-int(width/2))\n",
    "    new_image[start_row:start_row+height, start_col:start_col+width] = img[rmin:rmax+1,cmin:cmax+1]\n",
    "    new_mask[start_row:start_row+height, start_col:start_col+width] = mask[rmin:rmax+1,cmin:cmax+1]\n",
    "\n",
    "    #new_rmin = rcenter - max_dist\n",
    "    #new_rmax = rcenter + max_dist\n",
    "    #new_cmin = ccenter - max_dist\n",
    "    #new_cmax = ccenter + max_dist\n",
    "    new_image = new_image.astype('uint8')\n",
    "    new_mask = new_mask.astype('uint8')\n",
    "    return new_image, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "qwiCtbbLyD8d",
    "outputId": "a55babb4-5fbe-4150-b84f-4c0672c76295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/albu/albumentations\n",
      "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-bile5b6q\n",
      "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-bile5b6q\n",
      "Requirement already satisfied: numpy>=1.11.1 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from albumentations==1.0.0) (1.20.3)\n",
      "Requirement already satisfied: scipy in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from albumentations==1.0.0) (1.6.3)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from albumentations==1.0.0) (0.18.1)\n",
      "Requirement already satisfied: PyYAML in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from albumentations==1.0.0) (5.4.1)\n",
      "Requirement already satisfied: opencv-contrib-python>=4.1.1 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from albumentations==1.0.0) (4.5.2.54)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2021.6.14)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (8.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.5.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.4.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.3.1)\n",
      "Requirement already satisfied: six in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /playpen1/qiuyang/miniconda3/envs/qiuyang/lib/python3.8/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.0) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n",
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BiTmIy0AzwKA"
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
    "               'tree', 'signsymbol', 'fence', 'car', \n",
    "               'pedestrian', 'bicyclist', 'unlabelled']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            needBbox=False\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id.replace(\"img\", \"seg\")) for image_id in self.ids]\n",
    "\n",
    "        # convert str names to class values on masks\n",
    "        # self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]    \n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.needBbox = needBbox\n",
    "        self.resize = albu.Compose([albu.Resize(height=512, width=512)])\n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        # unsqueeze to (512, 512, 1)\n",
    "        mask = np.expand_dims(mask,-1)\n",
    "\n",
    "        if self.needBbox:\n",
    "          # cut out areas outside of bbox and pad with black pixels\n",
    "          image, mask = bbox2_padding(image, mask)\n",
    "          # resize to (512, 512) and change back type to int32\n",
    "          sample = self.resize(image=image, mask=mask)\n",
    "          image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            mask = mask.squeeze(0)\n",
    "        return image, mask\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "ZgIf1rRmJ0QP",
    "outputId": "c1487e19-fc90-405f-dc93-6e1172c15851"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_preprocessing(preprocessing_fn):\\n    \"\"\"Construct preprocessing transform\\n    \\n    Args:\\n        preprocessing_fn (callbale): data normalization function \\n            (can be specific for each pretrained neural network)\\n    Return:\\n        transform: albumentations.Compose\\n    \\n    \"\"\"\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor, mask=to_tensor),\\n    ]\\n    return albu.Compose(_transform)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.01, rotate_limit=10, shift_limit=0.0625, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albu.Resize(height=512, width=512)\n",
    "        #albu.RandomResizedCrop(height=512, width=512, scale=(0.8, 0.8))\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def to_image_scale(x, **kwargs):\n",
    "    x = x.astype(np.float32)\n",
    "    x = x /255.0\n",
    "    x = x - 0.5\n",
    "    x = x * 2\n",
    "    return x\n",
    "def to_mask_scale(x, **kwargs):\n",
    "    x  = x.astype(np.float32)\n",
    "    x = x/255.0\n",
    "    return x\n",
    "def get_transposed():\n",
    "    _transform = [\n",
    "      albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "      albu.Lambda(image=to_image_scale, mask=to_mask_scale),\n",
    "      albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "'''def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "j0sppT2zMJc1",
    "outputId": "aa4b9a87-fecc-42cc-c559-e4c1c53adc4a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8L-iGf-orBD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CidZ-95Ol8ic",
    "outputId": "50fe3fb3-8cef-4679-df6a-55941dee11c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder.conv1.weight', 'encoder.bn1.weight', 'encoder.bn1.bias', 'encoder.bn1.running_mean', 'encoder.bn1.running_var', 'encoder.bn1.num_batches_tracked', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.bn1.running_mean', 'encoder.layer1.0.bn1.running_var', 'encoder.layer1.0.bn1.num_batches_tracked', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.0.bn2.running_mean', 'encoder.layer1.0.bn2.running_var', 'encoder.layer1.0.bn2.num_batches_tracked', 'encoder.layer1.1.conv1.weight', 'encoder.layer1.1.bn1.weight', 'encoder.layer1.1.bn1.bias', 'encoder.layer1.1.bn1.running_mean', 'encoder.layer1.1.bn1.running_var', 'encoder.layer1.1.bn1.num_batches_tracked', 'encoder.layer1.1.conv2.weight', 'encoder.layer1.1.bn2.weight', 'encoder.layer1.1.bn2.bias', 'encoder.layer1.1.bn2.running_mean', 'encoder.layer1.1.bn2.running_var', 'encoder.layer1.1.bn2.num_batches_tracked', 'encoder.layer2.0.conv1.weight', 'encoder.layer2.0.bn1.weight', 'encoder.layer2.0.bn1.bias', 'encoder.layer2.0.bn1.running_mean', 'encoder.layer2.0.bn1.running_var', 'encoder.layer2.0.bn1.num_batches_tracked', 'encoder.layer2.0.conv2.weight', 'encoder.layer2.0.bn2.weight', 'encoder.layer2.0.bn2.bias', 'encoder.layer2.0.bn2.running_mean', 'encoder.layer2.0.bn2.running_var', 'encoder.layer2.0.bn2.num_batches_tracked', 'encoder.layer2.0.downsample.0.weight', 'encoder.layer2.0.downsample.1.weight', 'encoder.layer2.0.downsample.1.bias', 'encoder.layer2.0.downsample.1.running_mean', 'encoder.layer2.0.downsample.1.running_var', 'encoder.layer2.0.downsample.1.num_batches_tracked', 'encoder.layer2.1.conv1.weight', 'encoder.layer2.1.bn1.weight', 'encoder.layer2.1.bn1.bias', 'encoder.layer2.1.bn1.running_mean', 'encoder.layer2.1.bn1.running_var', 'encoder.layer2.1.bn1.num_batches_tracked', 'encoder.layer2.1.conv2.weight', 'encoder.layer2.1.bn2.weight', 'encoder.layer2.1.bn2.bias', 'encoder.layer2.1.bn2.running_mean', 'encoder.layer2.1.bn2.running_var', 'encoder.layer2.1.bn2.num_batches_tracked', 'encoder.layer3.0.conv1.weight', 'encoder.layer3.0.bn1.weight', 'encoder.layer3.0.bn1.bias', 'encoder.layer3.0.bn1.running_mean', 'encoder.layer3.0.bn1.running_var', 'encoder.layer3.0.bn1.num_batches_tracked', 'encoder.layer3.0.conv2.weight', 'encoder.layer3.0.bn2.weight', 'encoder.layer3.0.bn2.bias', 'encoder.layer3.0.bn2.running_mean', 'encoder.layer3.0.bn2.running_var', 'encoder.layer3.0.bn2.num_batches_tracked', 'encoder.layer3.0.downsample.0.weight', 'encoder.layer3.0.downsample.1.weight', 'encoder.layer3.0.downsample.1.bias', 'encoder.layer3.0.downsample.1.running_mean', 'encoder.layer3.0.downsample.1.running_var', 'encoder.layer3.0.downsample.1.num_batches_tracked', 'encoder.layer3.1.conv1.weight', 'encoder.layer3.1.bn1.weight', 'encoder.layer3.1.bn1.bias', 'encoder.layer3.1.bn1.running_mean', 'encoder.layer3.1.bn1.running_var', 'encoder.layer3.1.bn1.num_batches_tracked', 'encoder.layer3.1.conv2.weight', 'encoder.layer3.1.bn2.weight', 'encoder.layer3.1.bn2.bias', 'encoder.layer3.1.bn2.running_mean', 'encoder.layer3.1.bn2.running_var', 'encoder.layer3.1.bn2.num_batches_tracked', 'encoder.layer4.0.conv1.weight', 'encoder.layer4.0.bn1.weight', 'encoder.layer4.0.bn1.bias', 'encoder.layer4.0.bn1.running_mean', 'encoder.layer4.0.bn1.running_var', 'encoder.layer4.0.bn1.num_batches_tracked', 'encoder.layer4.0.conv2.weight', 'encoder.layer4.0.bn2.weight', 'encoder.layer4.0.bn2.bias', 'encoder.layer4.0.bn2.running_mean', 'encoder.layer4.0.bn2.running_var', 'encoder.layer4.0.bn2.num_batches_tracked', 'encoder.layer4.0.downsample.0.weight', 'encoder.layer4.0.downsample.1.weight', 'encoder.layer4.0.downsample.1.bias', 'encoder.layer4.0.downsample.1.running_mean', 'encoder.layer4.0.downsample.1.running_var', 'encoder.layer4.0.downsample.1.num_batches_tracked', 'encoder.layer4.1.conv1.weight', 'encoder.layer4.1.bn1.weight', 'encoder.layer4.1.bn1.bias', 'encoder.layer4.1.bn1.running_mean', 'encoder.layer4.1.bn1.running_var', 'encoder.layer4.1.bn1.num_batches_tracked', 'encoder.layer4.1.conv2.weight', 'encoder.layer4.1.bn2.weight', 'encoder.layer4.1.bn2.bias', 'encoder.layer4.1.bn2.running_mean', 'encoder.layer4.1.bn2.running_var', 'encoder.layer4.1.bn2.num_batches_tracked', 'decoder.blocks.0.conv1.0.weight', 'decoder.blocks.0.conv1.1.weight', 'decoder.blocks.0.conv1.1.bias', 'decoder.blocks.0.conv1.1.running_mean', 'decoder.blocks.0.conv1.1.running_var', 'decoder.blocks.0.conv1.1.num_batches_tracked', 'decoder.blocks.0.conv2.0.weight', 'decoder.blocks.0.conv2.1.weight', 'decoder.blocks.0.conv2.1.bias', 'decoder.blocks.0.conv2.1.running_mean', 'decoder.blocks.0.conv2.1.running_var', 'decoder.blocks.0.conv2.1.num_batches_tracked', 'decoder.blocks.1.conv1.0.weight', 'decoder.blocks.1.conv1.1.weight', 'decoder.blocks.1.conv1.1.bias', 'decoder.blocks.1.conv1.1.running_mean', 'decoder.blocks.1.conv1.1.running_var', 'decoder.blocks.1.conv1.1.num_batches_tracked', 'decoder.blocks.1.conv2.0.weight', 'decoder.blocks.1.conv2.1.weight', 'decoder.blocks.1.conv2.1.bias', 'decoder.blocks.1.conv2.1.running_mean', 'decoder.blocks.1.conv2.1.running_var', 'decoder.blocks.1.conv2.1.num_batches_tracked', 'decoder.blocks.2.conv1.0.weight', 'decoder.blocks.2.conv1.1.weight', 'decoder.blocks.2.conv1.1.bias', 'decoder.blocks.2.conv1.1.running_mean', 'decoder.blocks.2.conv1.1.running_var', 'decoder.blocks.2.conv1.1.num_batches_tracked', 'decoder.blocks.2.conv2.0.weight', 'decoder.blocks.2.conv2.1.weight', 'decoder.blocks.2.conv2.1.bias', 'decoder.blocks.2.conv2.1.running_mean', 'decoder.blocks.2.conv2.1.running_var', 'decoder.blocks.2.conv2.1.num_batches_tracked', 'decoder.blocks.3.conv1.0.weight', 'decoder.blocks.3.conv1.1.weight', 'decoder.blocks.3.conv1.1.bias', 'decoder.blocks.3.conv1.1.running_mean', 'decoder.blocks.3.conv1.1.running_var', 'decoder.blocks.3.conv1.1.num_batches_tracked', 'decoder.blocks.3.conv2.0.weight', 'decoder.blocks.3.conv2.1.weight', 'decoder.blocks.3.conv2.1.bias', 'decoder.blocks.3.conv2.1.running_mean', 'decoder.blocks.3.conv2.1.running_var', 'decoder.blocks.3.conv2.1.num_batches_tracked', 'decoder.blocks.4.conv1.0.weight', 'decoder.blocks.4.conv1.1.weight', 'decoder.blocks.4.conv1.1.bias', 'decoder.blocks.4.conv1.1.running_mean', 'decoder.blocks.4.conv1.1.running_var', 'decoder.blocks.4.conv1.1.num_batches_tracked', 'decoder.blocks.4.conv2.0.weight', 'decoder.blocks.4.conv2.1.weight', 'decoder.blocks.4.conv2.1.bias', 'decoder.blocks.4.conv2.1.running_mean', 'decoder.blocks.4.conv2.1.running_var', 'decoder.blocks.4.conv2.1.num_batches_tracked', 'segmentation_head.0.weight', 'segmentation_head.0.bias'])"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "wi9c9QIT7Gzn",
    "outputId": "aea5cf5a-64e0-486c-da72-05cbf396d8ef"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-058eb7449a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#needBbox = True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-488725a3a577>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images_dir, masks_dir, classes, augmentation, preprocessing, needBbox)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mneedBbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     ):\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected: '/content/drive/MyDrive/Henry_Lab/Cap08102020_seg/val'"
     ]
    }
   ],
   "source": [
    "# # Use pretrained model to predict (without bbox)\n",
    "# new_model = smp.Unet(\n",
    "#             'resnet18',\n",
    "#             encoder_weights='imagenet', \n",
    "#             classes=2, \n",
    "#             # activation='softmax'\n",
    "#         ).to(device)\n",
    "\n",
    "# new_model.load_state_dict(state_dict2)\n",
    "# test_dataset = Dataset(\n",
    "#     x_valid_dir, \n",
    "#     y_valid_dir,  \n",
    "#     preprocessing=get_preprocessing(),\n",
    "#     #needBbox = True,\n",
    "#     classes=CLASSES,\n",
    "# )\n",
    "\n",
    "# test_dataset_vis = Dataset(\n",
    "#     x_valid_dir, y_valid_dir, \n",
    "#     #needBbox = True,\n",
    "#     classes=CLASSES,\n",
    "# )\n",
    "# for i in range(5):\n",
    "#   n = np.random.choice(len(test_dataset))\n",
    "\n",
    "#   image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "#   image, gt_mask = test_dataset[n]\n",
    "\n",
    "#   gt_mask = gt_mask.squeeze()\n",
    "\n",
    "#   x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "#   #pr_mask =  model.predict(x_tensor)\n",
    "#   pr_mask =  new_model.predict(x_tensor)\n",
    "#   pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "#   mask = pr_mask[1] > pr_mask[0]\n",
    "#   new_img = np.zeros(gt_mask.shape)\n",
    "#   new_img[mask] = 1\n",
    "\n",
    "#   visualize(\n",
    "#       image=image_vis,\n",
    "#       ground_truth_mask=gt_mask, \n",
    "#       mask1 = pr_mask[0],\n",
    "#       mask2 = pr_mask[1],\n",
    "#       seg_image=new_img\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APHR3qbL5zu6"
   },
   "outputs": [],
   "source": [
    "# # Use pretrained model to predict (withbbox)\n",
    "# new_model = smp.Unet(\n",
    "#             'resnet18',\n",
    "#             encoder_weights='imagenet', \n",
    "#             classes=2, \n",
    "#             # activation='softmax'\n",
    "#         ).to(device)\n",
    "\n",
    "# new_model.load_state_dict(state_dict2)\n",
    "# test_dataset = Dataset(\n",
    "#     x_valid_dir, \n",
    "#     y_valid_dir,  \n",
    "#     preprocessing=get_preprocessing(),\n",
    "#     needBbox = True,\n",
    "#     classes=CLASSES,\n",
    "# )\n",
    "\n",
    "# test_dataset_vis = Dataset(\n",
    "#     x_valid_dir, y_valid_dir, \n",
    "#     needBbox = True,\n",
    "#     classes=CLASSES,\n",
    "# )\n",
    "# for i in range(5):\n",
    "#   n = np.random.choice(len(test_dataset))\n",
    "\n",
    "#   image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "#   image, gt_mask = test_dataset[n]\n",
    "\n",
    "#   gt_mask = gt_mask.squeeze()\n",
    "\n",
    "#   x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "#   #pr_mask =  model.predict(x_tensor)\n",
    "#   pr_mask =  new_model.predict(x_tensor)\n",
    "#   pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "#   mask = pr_mask[1] > pr_mask[0]\n",
    "#   new_img = np.zeros(gt_mask.shape)\n",
    "#   new_img[mask] = 1\n",
    "\n",
    "#   visualize(\n",
    "#       image=image_vis,\n",
    "#       ground_truth_mask=gt_mask, \n",
    "#       seg_image=new_img\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSMGUNT5dti6"
   },
   "outputs": [],
   "source": [
    "pretrained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KB7gCgGsjhQ",
    "outputId": "6e37ac97-352f-4ea8-cfea-722cc55c8582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (1.10.0.post2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzbkuYsMsFW9",
    "outputId": "b57b522e-106f-4f91-e798-3dd15bdc8446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "fatal: destination path 'Self-Correction-Human-Parsing' already exists and is not an empty directory.\n",
      "/content/Self-Correction-Human-Parsing\n",
      "mkdir: cannot create directory ‘checkpoints’: File exists\n",
      "mkdir: cannot create directory ‘inputs’: File exists\n",
      "mkdir: cannot create directory ‘outputs’: File exists\n"
     ]
    }
   ],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing\n",
    "%cd Self-Correction-Human-Parsing\n",
    "!mkdir checkpoints\n",
    "!mkdir inputs\n",
    "!mkdir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "jUp6v4kOsIFk",
    "outputId": "2d8724f4-14c9-44a3-f9ad-61a361169607"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1E5YwNKW2VOEayK9mWCS3Kpsxf-3z04ZE\n",
      "To: /content/Self-Correction-Human-Parsing/checkpoints/final.pth\n",
      "267MB [00:01, 167MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'checkpoints/final.pth'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'pascal'         #select from ['lip', 'atr', 'pascal']\n",
    "import gdown\n",
    "\n",
    "if dataset == 'lip':\n",
    "    url = 'https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH'\n",
    "elif dataset == 'atr':\n",
    "    url = 'https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP'\n",
    "elif dataset == 'pascal':\n",
    "    url = 'https://drive.google.com/uc?id=1E5YwNKW2VOEayK9mWCS3Kpsxf-3z04ZE'\n",
    "\n",
    "output = 'checkpoints/final.pth'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FMsEOGtJsPJJ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f53d2e5b0e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#device setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#dataset_settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networks'"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "#device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#dataset_settings\n",
    "input_size = [512, 512]\n",
    "num_classes = 7\n",
    "label = ['Background', 'Head', 'Torso', 'Upper Arms', 'Lower Arms', 'Upper Legs', 'Lower Legs']\n",
    "\n",
    "model = networks.init_model('resnet101', num_classes=num_classes, pretrained=None)\n",
    "state_dict = torch.load('/content/Self-Correction-Human-Parsing/checkpoints/final.pth')['state_dict']\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]  # remove `module.`\n",
    "    new_state_dict[name] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "eW_OiDbFeJyz",
    "outputId": "6d4a6d64-556e-4e97-860a-301203417ba5"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ddc92f82c868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mx_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;31m#prob = model(x_tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mupsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/Self-Correction-Human-Parsing/networks/AugmentCE2P.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 13.67 GiB already allocated; 3.75 MiB free; 13.72 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# fine-tune the pretrained_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing = get_preprocessing(),\n",
    "    needBbox = True,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "test_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir,  \n",
    "    preprocessing=get_preprocessing(),\n",
    "    needBbox = True,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataset_vis = Dataset(\n",
    "    x_valid_dir, y_valid_dir, \n",
    "    #needBbox = True,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "#initialize dataloader\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# initialize parameters\n",
    "loss_Softmax = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "max_score = 0\n",
    "best_epoch = 0\n",
    "best_iou = 0\n",
    "training_loss = []\n",
    "\n",
    "# def train\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  model.train()\n",
    "  running_loss = 0\n",
    "  iou = 0\n",
    "  for batch_idx, (input, target) in enumerate(train_loader):\n",
    "      input, target = input.to(device),target.to(device, dtype=torch.int64)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(input)\n",
    "      upsample = torch.nn.Upsample(size=input_size, mode='bilinear', align_corners=True)\n",
    "      upsample_output = upsample(output[0][-1])\n",
    "      logit0 = upsample_output[:, 0, :, :]\n",
    "      logit1 = torch.sum(upsample_output[:, 1:, :, :], 1)\n",
    "      pr_mask = torch.stack((logit0, logit1), 1)\n",
    "      # pred = output[:, 1, :, :].detach().float()\n",
    "      # pred = pred > 0.5\n",
    "      # label = target > 0.5\n",
    "      # iou += ((pred & label).sum((1, 2)).sum() + 1e-6) / ((pred | label).sum((1, 2)).sum() + 1e-6) \n",
    "      loss = loss_Softmax(pr_mask, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "      # print every 15 mini-batches\n",
    "  print('[%d, %5d] loss: %.3f' %\n",
    "        (epoch, batch_idx + 1, running_loss / batch_idx + 1))\n",
    "  print(f\"iou: {iou / len(train_loader.dataset) * pred.size(0)}\")\n",
    "  return iou\n",
    "# load model\n",
    "# model.load_state_dict(state_dict2)\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "# train model for 40 epochs\n",
    "epoches = np.arange(0, 10)\n",
    "for i in epoches:  \n",
    "    image, gt_mask = test_dataset[0]\n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    output = model(x_tensor)\n",
    "    #prob = model(x_tensor)\n",
    "    upsample = torch.nn.Upsample(size=input_size, mode='bilinear', align_corners=True)\n",
    "    upsample_output = upsample(output[0][-1])\n",
    "    logit0 = upsample_output[:, 0, :, :]\n",
    "    logit1 = torch.sum(upsample_output[:, 1:, :, :], 1)\n",
    "    pr_mask = torch.stack((logit0, logit1), 1)\n",
    "    pr_mask = pr_mask.softmax(dim=1).argmax(dim=1)\n",
    "    pr_mask = (pr_mask != 0)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round().astype('uint8'))\n",
    "    visualize(img=pr_mask)\n",
    "    \n",
    "\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    print(f\"lr: {optimizer.param_groups[0]['lr']}\")\n",
    "    iou = train(model, device, train_dataloader, optimizer, i)\n",
    "    # do something (save model, change lr, etc.)\n",
    "    if iou > best_iou:\n",
    "      best_epoch = i+1\n",
    "      best_iou = iou\n",
    "      torch.save(model, os.path.join(DATA_DIR, 'best_model%d.pth'% (i+1)))\n",
    "      print('Model saved!')\n",
    "    '''if max_score < train_logs['iou_score']:\n",
    "        best_epoch = i\n",
    "        max_score = train_logs['iou_score']\n",
    "        torch.save(model, os.path.join(DATA_DIR, 'best_model.pth'))\n",
    "        print('Model saved!')'''\n",
    "        \n",
    "    '''if i % 1 == 0:\n",
    "        image, gt_mask = test_dataset[0]\n",
    "        gt_mask = gt_mask.squeeze()\n",
    "        x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "        pr_mask = model.predict(x_tensor)\n",
    "        pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "        mask = pr_mask[1] > pr_mask[0]\n",
    "        new_img = np.zeros(gt_mask.shape)\n",
    "        new_img[mask] = 1\n",
    "        visualize(mask = gt_mask, seg_image=new_img)'''\n",
    "\n",
    "    scheduler.step()\n",
    "  \n",
    "#print(f\"Best epoch is {best_epoch}\")\n",
    "#plt.plot(epoches, training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHYOjo7M_xrq"
   },
   "outputs": [],
   "source": [
    " best_model = torch.load(os.path.join(DATA_DIR, 'best_model8.pth'))\n",
    "test_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir,  \n",
    "    preprocessing=get_preprocessing(),\n",
    "    needBbox = True,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataset_vis = Dataset(\n",
    "    x_valid_dir, y_valid_dir, \n",
    "    #needBbox = True,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "for i in range(5):\n",
    "  n = np.random.choice(len(test_dataset))\n",
    "\n",
    "  image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "  image, gt_mask = test_dataset[n]\n",
    "\n",
    "  gt_mask = gt_mask.squeeze()\n",
    "\n",
    "  x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "  #pr_mask =  model.predict(x_tensor)\n",
    "  pr_mask =  best_model.predict(x_tensor)\n",
    "  pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "  mask = pr_mask[1] > pr_mask[0]\n",
    "  new_img = np.zeros(gt_mask.shape)\n",
    "  new_img[mask] = 1\n",
    "\n",
    "  visualize(\n",
    "      image=image_vis,\n",
    "      ground_truth_mask=gt_mask, \n",
    "      seg_image=new_img\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebgQ0FcRgBtX"
   },
   "outputs": [],
   "source": [
    "# Use fine-tuned model to predict\n",
    "'''test_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir,  \n",
    "    preprocessing=get_preprocessing(),\n",
    "    needBbox = True,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataset_vis = Dataset(\n",
    "    x_valid_dir, y_valid_dir, \n",
    "    classes=CLASSES,\n",
    ")\n",
    "for i in range(5):\n",
    "  n = np.random.choice(len(test_dataset))\n",
    "\n",
    "  image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "  image, gt_mask = test_dataset[n]\n",
    "\n",
    "  gt_mask = gt_mask.squeeze()\n",
    "\n",
    "  x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "  pr_mask =  model.predict(x_tensor)\n",
    "  #pr_mask =  pretrained_model.predict(x_tensor)\n",
    "  pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "  mask = pr_mask[1] > pr_mask[0]\n",
    "  new_img = np.zeros(gt_mask.shape)\n",
    "  new_img[mask] = 1\n",
    "\n",
    "  visualize(\n",
    "      image=image_vis,\n",
    "      ground_truth_mask=gt_mask, \n",
    "      seg_image=new_img\n",
    "  )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2g87vUzh-8G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "June 17 Correct Copy of toy sample image segmentation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
